# ScriptGuard FAST TEST Configuration - ULTRA-MINIMAL for rapid pipeline verification
# Optimized for fastest possible execution to verify pipeline functionality

# Pipeline Configuration
pipeline:
  enable_cache: false  # Disabled - always run fresh
  cache_steps:
    advanced_data_ingestion: false
    validate_samples: false
    augment_malicious_samples: false
    train_model: false

# API Keys (loaded from environment variables)
api_keys:
  github_token: ${GITHUB_API_TOKEN}
  nvd_api_key: ${NVD_API_KEY}
  malwarebazaar_api_key: ${MALWAREBAZAAR_API_KEY}
  huggingface_token: ${HUGGINGFACE_TOKEN}

# Data Sources Configuration - ULTRA-MINIMAL
data_sources:
  github:
    enabled: true
    fetch_malicious: true
    fetch_benign: true
    malicious_keywords:
      - "reverse-shell python"
    benign_repos:
      - "pallets/flask"
    max_samples_per_keyword: 2  # Can override with MAX_SAMPLES_PER_KEYWORD env var in config.yaml
    max_files_per_repo: 2       # Can override with MAX_FILES_PER_REPO env var in config.yaml

  malwarebazaar:
    enabled: false
    tags: []
    max_samples: 0

  huggingface:
    enabled: false
    datasets: []

  cve_feeds:
    enabled: false
    feeds: []

  additional_hf:
    enabled: false
    datasets: []

# Validation Configuration - Relaxed for speed
validation:
  validate_syntax: false
  min_length: ${MIN_LENGTH:-10}
  max_length: ${MAX_LENGTH:-100000}
  min_code_lines: ${MIN_CODE_LINES:-1}
  max_comment_ratio: ${MAX_COMMENT_RATIO:-0.95}

# Augmentation Configuration - MINIMAL
augmentation:
  enabled: false
  variants_per_sample: ${VARIANTS_PER_SAMPLE:-0}
  balance_dataset: false
  target_balance_ratio: ${TARGET_BALANCE_RATIO:-1.0}
  balance_method: ${BALANCE_METHOD:-undersample}
  use_qdrant_patterns: false

# Qdrant Configuration
qdrant:
  host: ${QDRANT_HOST:-localhost}
  port: ${QDRANT_PORT:-6333}
  api_key: ${QDRANT_API_KEY:-}
  collection_name: ${QDRANT_COLLECTION:-malware_knowledge_test}
  cve_collection: ${QDRANT_CVE_COLLECTION:-cve_patterns_test}

# Code Embedding Configuration - MINIMAL
code_embedding:
  model: ${EMBEDDING_MODEL:-microsoft/unixcoder-base}
  embedding_dim: ${EMBEDDING_DIM:-768}
  chunk_size: ${CHUNK_SIZE:-256}
  chunk_overlap: ${CHUNK_OVERLAP:-32}
  max_samples_to_vectorize: ${MAX_SAMPLES_TO_VECTORIZE:-0}
  enable_chunking: false  # CRITICAL: Disable chunking for speed
  max_code_length: 512    # Single embedding per sample

# Training Configuration - ULTRA-MINIMAL
training:
  model_id: ${MODEL_ID:-bigcode/starcoder2-3b}
  output_dir: ${OUTPUT_DIR:-./model_checkpoints_test}
  num_epochs: ${NUM_EPOCHS:-1}
  batch_size: ${BATCH_SIZE:-1}
  gradient_accumulation_steps: ${GRAD_ACCUM_STEPS:-1}
  learning_rate: ${LEARNING_RATE:-0.0002}
  max_seq_length: ${MAX_SEQ_LENGTH:-256}
  warmup_steps: ${WARMUP_STEPS:-1}
  logging_steps: ${LOGGING_STEPS:-5}
  save_steps: ${SAVE_STEPS:-500}
  eval_steps: ${EVAL_STEPS:-500}
  save_total_limit: ${SAVE_TOTAL_LIMIT:-1}
  fp16: false
  bf16: true  # Enable BF16 for faster training on modern GPUs
  test_split_size: ${TEST_SPLIT_SIZE:-0.5}
  max_steps: ${MAX_STEPS:-10}

  # Flash Attention 2 Configuration
  use_flash_attention_2: true  # 2x faster, 50% less memory
  attn_implementation: "flash_attention_2"
  # Note: attention_dropout is automatically set to 0.0 in qlora_finetuner.py

  # LoRA Configuration - Minimal
  lora_r: ${LORA_R:-8}
  lora_alpha: ${LORA_ALPHA:-16}
  lora_dropout: ${LORA_DROPOUT:-0.05}
  target_modules:
    - "q_proj"
    - "v_proj"

  # Evaluation settings - Minimal
  eval_max_new_tokens: ${EVAL_MAX_NEW_TOKENS:-10}
  eval_temperature: ${EVAL_TEMPERATURE:-0.1}
  eval_max_code_length: ${EVAL_MAX_CODE_LENGTH:-200}

# W&B Configuration
wandb:
  enabled: false
  project: ${WANDB_PROJECT:-scriptguard-test}
  entity: ${WANDB_ENTITY:-your-wandb-entity}

# Database Configuration
database:
  enabled: false
  host: ${DB_HOST:-localhost}
  port: ${DB_PORT:-5432}
  name: ${DB_NAME:-scriptguard_test}
  user: ${DB_USER:-scriptguard}
  password: ${DB_PASSWORD:-your-secure-password}
