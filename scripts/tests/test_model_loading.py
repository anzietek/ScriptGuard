"""
Test sprawdzajƒÖcy czy model evaluation mo≈ºe za≈Çadowaƒá model bez b≈Çƒôd√≥w
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_model_loading():
    """Test czy model ≈Çaduje siƒô bez b≈Çƒôdu Params4bit"""

    base_model_id = "bigcode/starcoder2-3b"

    print("=" * 60)
    print("TEST: Model Loading for Evaluation")
    print("=" * 60)

    # Test 1: Sprawd≈∫ czy GPU loading z float16 dzia≈Ça
    print("\n‚úì Test 1: GPU with float16 and memory management...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map="auto",
            max_memory={0: "3.5GB"}  # Reserve some VRAM for operations
        )
        print("  ‚úÖ GPU float16 with memory management - OK")
        print(f"  Device map: {model.hf_device_map}")
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        return True
    except Exception as e:
        print(f"  ‚ö†Ô∏è GPU with memory limit failed: {type(e).__name__}: {str(e)[:100]}")

    # Test 2: Sprawd≈∫ czy loading bez quantization dzia≈Ça
    print("\n‚úì Test 2: GPU without quantization...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        print("  ‚úÖ GPU fp16 - OK")
        print(f"  Device: {model.device}")
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        return True
    except Exception as e:
        print(f"  ‚ö†Ô∏è GPU fp16 failed: {type(e).__name__}: {str(e)[:100]}")

    # Test 3: CPU fallback
    print("\n‚úì Test 3: CPU fallback...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            device_map="cpu",
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )
        print("  ‚úÖ CPU fp32 - OK")
        print(f"  Device: {model.device}")
        del model
        return True
    except Exception as e:
        print(f"  ‚ùå CPU failed: {type(e).__name__}: {str(e)[:100]}")
        return False

if __name__ == "__main__":
    print("üß™ Testing model loading strategies...")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(0)}")
        print(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    success = test_model_loading()

    print("\n" + "=" * 60)
    if success:
        print("‚úÖ MODEL LOADING TEST PASSED!")
        print("Model evaluation powinien teraz dzia≈Çaƒá bez b≈Çƒôd√≥w.")
    else:
        print("‚ùå MODEL LOADING TEST FAILED!")
        print("Model mo≈ºe nie ≈Çadowaƒá siƒô - sprawd≈∫ instalacje transformers/bitsandbytes")
    print("=" * 60)
